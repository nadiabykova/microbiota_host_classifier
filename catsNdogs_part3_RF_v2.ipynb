{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from skbio.stats.composition import clr\n",
    "from sklearn import model_selection, ensemble, metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import shared_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catNdog    321\n",
      "human      321\n",
      "Name: Host_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bf = joblib.load('joblib/catsNdogs_mw_bf.joblib')\n",
    "data = joblib.load('joblib/catsNdogs_data.joblib')\n",
    "dataset_info = joblib.load('joblib/catsNdogs_dataset_info.joblib')\n",
    "print(dataset_info.Host_type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameters(X,y,name):\n",
    "    max_features = int(np.sqrt(X.shape[1]))+1\n",
    "    param_grid = {'n_estimators': [100],\n",
    "                  'max_depth': np.arange(2,12,1),\n",
    "                  'min_samples_split': np.arange(2,11,1),\n",
    "                  'max_features': np.arange(1,max_features+1)\n",
    "                 }\n",
    "    rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "    optimizer = model_selection.GridSearchCV(rf, param_grid, cv=5, n_jobs=4,return_train_score=True)\n",
    "    optimizer.fit(X, y)\n",
    "    best_score1 = optimizer.best_score_\n",
    "    best_params1 = optimizer.best_params_\n",
    "    print('Best score 1:',best_score1)\n",
    "    print('Best parameters 1:',best_params1)\n",
    "    param_grid_big = {'n_estimators': [1,5,10,50,100,500,1000],\n",
    "                      'max_depth': np.arange(2,30,1),\n",
    "                      'min_samples_split': np.arange(2,30,1),\n",
    "                      'max_features': np.arange(1,X.shape[1]+1)\n",
    "                     }\n",
    "    best_params2 = {}\n",
    "    n_cols = 2\n",
    "    n_rows = int(np.ceil((len(best_params1))/n_cols))\n",
    "    fig, axs = plt.subplots(n_rows,n_cols,figsize=(n_cols*5,n_rows*5))\n",
    "    fig.suptitle(name)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for k1,v1 in best_params1.items():\n",
    "        k_dict = {}\n",
    "        for k,v in best_params1.items():\n",
    "            if (k==k1):\n",
    "                k_dict[k]=param_grid_big[k]\n",
    "               # k_dict[k]=param_grid[k]\n",
    "            else:\n",
    "                k_dict[k] = [best_params1[k]]\n",
    "        rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "        optimizer = model_selection.GridSearchCV(rf, k_dict, cv=5, n_jobs=4,return_train_score=True)\n",
    "        optimizer.fit(X, y)\n",
    "        best_params2[k1] = optimizer.best_params_[k1]\n",
    "        if (k1 in ['n_estimators','min_impurity_decrease']):\n",
    "            plot_scores(optimizer,k1,axs[i][j],log=True)\n",
    "            axs[i][j].title.set_text(k1)\n",
    "        else:\n",
    "            plot_scores(optimizer,k1,axs[i][j],log=False)\n",
    "            axs[i][j].title.set_text(k1)\n",
    "        j = j + 1\n",
    "        if (j==axs.shape[1]):\n",
    "            i = i + 1\n",
    "            j = 0\n",
    "    plt.show()\n",
    "    print('Best parameters 2',best_params2)\n",
    "    return(best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(optimizer,p_name,ax,log=False):\n",
    "    scores=[]\n",
    "    for i in range(len(optimizer.cv_results_['params'])):\n",
    "        scores.append([optimizer.cv_results_['params'][i][p_name], \n",
    "                optimizer.cv_results_['mean_test_score'][i],\n",
    "                optimizer.cv_results_['std_test_score'][i]])\n",
    "    scores = np.array(scores)\n",
    "    if (log):\n",
    "        ax.semilogx(scores[:,0], scores[:,1],label='test')\n",
    "    else:\n",
    "        ax.plot(scores[:,0], scores[:,1],label='test')\n",
    "    ax.fill_between(scores[:,0], scores[:,1]-scores[:,2], \n",
    "                                  scores[:,1]+scores[:,2], alpha=0.3)\n",
    "    \n",
    "    scores2=[]\n",
    "    for i in range(len(optimizer.cv_results_['params'])):\n",
    "        scores2.append([optimizer.cv_results_['params'][i][p_name], \n",
    "                optimizer.cv_results_['mean_train_score'][i],\n",
    "                optimizer.cv_results_['std_train_score'][i]])\n",
    "    scores2 = np.array(scores2)\n",
    "    if (log):\n",
    "        ax.semilogx(scores2[:,0], scores2[:,1],label='train')\n",
    "    else:\n",
    "        ax.plot(scores2[:,0], scores2[:,1],label='train')\n",
    "    ax.fill_between(scores2[:,0], scores2[:,1]-scores2[:,2], \n",
    "                                  scores2[:,1]+scores2[:,2], alpha=0.3)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cv(rf,X,y,name):\n",
    "    sss = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=0)\n",
    "    plt.figure(figsize=(6,6));\n",
    "    plt.title(name)\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    p_names = ['accuracy','precision','recall','f1','auc','tpr']\n",
    "    tt = ['train','test']\n",
    "    cv_results = {}\n",
    "    for p,t in itertools.product(p_names, tt):\n",
    "        cv_results[(p,t)]=[]\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rf.fit(X_train,y_train)\n",
    "        y_predicted = rf.predict(X_test)\n",
    "        y_predicted_train = rf.predict(X_train)\n",
    "        y_proba = rf.predict_proba(X_test).transpose()[1]\n",
    "        y_proba_train = rf.predict_proba(X_train).transpose()[1]\n",
    "        \n",
    "        cv_results[('accuracy','test')].append(metrics.accuracy_score(y_test, y_predicted))\n",
    "        cv_results[('accuracy','train')].append(metrics.accuracy_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('f1','test')].append(metrics.f1_score(y_test, y_predicted))\n",
    "        cv_results[('f1','train')].append(metrics.f1_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('precision','test')].append(metrics.precision_score(y_test, y_predicted))\n",
    "        cv_results[('precision','train')].append(metrics.precision_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('recall','test')].append(metrics.recall_score(y_test, y_predicted))\n",
    "        cv_results[('recall','train')].append(metrics.recall_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('auc','test')].append(metrics.roc_auc_score(y_test, y_proba))\n",
    "        cv_results[('auc','train')].append(metrics.roc_auc_score(y_train, y_proba_train))\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_proba)\n",
    "        fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_proba_train)\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr_train = np.interp(base_fpr, fpr_train, tpr_train)\n",
    "        tpr[0] = 0.0\n",
    "        tpr_train[0] = 0.0\n",
    "        cv_results[('tpr','test')].append(tpr)\n",
    "        cv_results[('tpr','train')].append(tpr_train)\n",
    "    col = {'train':'c','test':'m'}\n",
    "    for t in tt:\n",
    "        t_tprs = np.array(cv_results[('tpr',t)])\n",
    "        mean = t_tprs.mean(axis=0)\n",
    "        std = t_tprs.std(axis=0)\n",
    "        \n",
    "        mean_auc = np.mean(cv_results[('auc',t)])\n",
    "        std_auc = np.std(cv_results[('auc',t)])\n",
    "        \n",
    "        tprs_upper = np.minimum(mean + std, 1)\n",
    "        tprs_lower = mean - std\n",
    "        \n",
    "        plt.plot(base_fpr, mean, color = col[t], alpha = 0.8, label=t+r' (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc))\n",
    "        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = col[t], alpha = 0.2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    p_names = ['accuracy','precision','recall','f1','auc']\n",
    "    res = {}\n",
    "    for p in p_names:\n",
    "        mean = np.mean(cv_results[(p,t)])\n",
    "        std = np.std(cv_results[(p,t)])\n",
    "        mean_std = '%.3f Â± %.3f' % (mean, std)\n",
    "        print(p,mean_std)\n",
    "        res[(p+'_cv')] = mean_std\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oob_estimate(X,y,bp):\n",
    "    oob_df_list = []\n",
    "    for i in range(100):\n",
    "        rf = ensemble.RandomForestClassifier(random_state=i,oob_score=True)\n",
    "        rf.set_params(**bp)\n",
    "        rf.fit(X,y)\n",
    "        prob_0=rf.oob_decision_function_.transpose()[0]\n",
    "        y_predicted = [0 if x>0.5 else 1 for x in prob_0]\n",
    "        oob_df_list.append({'oob_accuracy':metrics.accuracy_score(y,y_predicted),\n",
    "                            'oob_f1_score':metrics.f1_score(y,y_predicted),\n",
    "                            'oob_precision':metrics.precision_score(y,y_predicted),\n",
    "                            'oob_recall':metrics.recall_score(y,y_predicted)})\n",
    "    oob_df = pd.DataFrame(oob_df_list)\n",
    "    means = oob_df.mean(axis=0)\n",
    "    stds = oob_df.std(axis=0)\n",
    "    oob_df_avg = pd.DataFrame.from_dict({'means':means,'stds':stds})\n",
    "    oob_df_avg['mean_std'] = oob_df_avg.apply(lambda x: ('%.3f Â± %.3f' % (x.means, x.stds)),axis=1)\n",
    "    return(oob_df_avg['mean_std'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters search, best parametrs model CV on train, and evaluation on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = pd.DataFrame()\n",
    "levels = [4,5]\n",
    "features = ['all','best_holm','best_fdr']\n",
    "clr_b = [True, False]\n",
    "#levels = [5]\n",
    "#features = ['all']\n",
    "#clr_b = [False]\n",
    "#level = levels[0]\n",
    "#features = features[0]\n",
    "#clr_b = clr_b[0]\n",
    "for level,features,clr_b in itertools.product(levels, features, clr_b):\n",
    "    view_name = shared_functions.view_name(level,features,clr_b)\n",
    "    print('Model:',view_name,'(tax level:',shared_functions.get_tax_name_by_level(level),\n",
    "          '; features:',features,'; CLR:',clr_b,')')\n",
    "    f,chao = bf[(level,features)]\n",
    "    tf = shared_functions.transformer(bf=f,chao=chao,level=level,clr_b=clr_b)\n",
    "    taxa_df,chao_df,y = data.get_data_from_ind(dataset_info.index,False)\n",
    "    X = tf.transform_df(taxa_df,chao_df)\n",
    "    bp = test_parameters(X,y,view_name)\n",
    "    rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "    rf.set_params(**bp)\n",
    "    name = shared_functions.make_name(level,features,clr_b,False)\n",
    "    joblib.dump(rf, 'joblib/'+name)\n",
    "    print(view_name+' CV(=5) estimation on train dataset')\n",
    "    res_dict = test_cv(rf,X,y,view_name)\n",
    "    rf.fit(X,y)\n",
    "    name = shared_functions.make_name(level,features,clr_b,True)\n",
    "    joblib.dump(rf, 'joblib/'+name)\n",
    "    oob_res_dict = get_oob_estimate(X,y,bp)\n",
    "    print(oob_res_dict)\n",
    "    p = {'name':view_name,'level':shared_functions.get_tax_name_by_level(level),'features':features,'features_n':X.shape[1],'CLR':clr_b}\n",
    "    z = {**p, **bp, **res_dict,**oob_res_dict}\n",
    "    res_i = pd.DataFrame(z,index=[0])\n",
    "    res = pd.concat([res,res_i])\n",
    "    print('*****************')\n",
    "res.to_csv('results/catsNdogs_Table4_models_stat.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
