{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from skbio.stats.composition import clr\n",
    "from sklearn import model_selection, ensemble, metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import fpdf\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 6,'axes.labelsize':8,'xtick.labelsize':8,'ytick.labelsize':8})\n",
    "\n",
    "import shared_functions\n",
    "joblib_dir = 'joblib_test'\n",
    "pic_dir = 'pic_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human    321\n",
      "pet      321\n",
      "Name: Host_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bf = joblib.load('joblib/catsNdogs_mw_bf.joblib')\n",
    "data = joblib.load('joblib/catsNdogs_data.joblib')\n",
    "dataset_info = joblib.load('joblib/catsNdogs_dataset_info.joblib')\n",
    "print(dataset_info.Host_type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_parameters(X,y,name,pdf):\n",
    "#     max_features = int(np.sqrt(X.shape[1]))+1\n",
    "#     param_grid = {'n_estimators': [100],\n",
    "#                   'max_depth': np.arange(2,12,1),\n",
    "#                   'min_samples_split': np.arange(2,11,1),\n",
    "#                   'max_features': np.arange(1,max_features+1)\n",
    "#                  }\n",
    "# #     param_grid = {'n_estimators': [100],\n",
    "# #                   'max_depth': np.arange(2,4,1),\n",
    "# #                   'min_samples_split': np.arange(2,4,1),\n",
    "# #                   'max_features': np.arange(1,4)\n",
    "# #                  }\n",
    "#     rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "#     optimizer = model_selection.GridSearchCV(rf, param_grid, cv=5, n_jobs=4,return_train_score=True)\n",
    "#     optimizer.fit(X, y)\n",
    "#     best_score1 = optimizer.best_score_\n",
    "#     best_params1 = optimizer.best_params_\n",
    "#     print('Best score 1:',best_score1)\n",
    "#     print('Best parameters 1:',best_params1)\n",
    "#     param_grid_big = {'n_estimators': [1,5,10,50,100,500,1000],\n",
    "#                       'max_depth': np.arange(2,31,1),\n",
    "#                       'min_samples_split': np.arange(2,31,1),\n",
    "#                       'max_features': np.arange(1,X.shape[1]+1)\n",
    "#                      }\n",
    "#     best_params2 = {}\n",
    "#     n_cols = 2\n",
    "#     n_rows = int(np.ceil((len(best_params1))/n_cols))\n",
    "#     fig, axs = plt.subplots(n_rows,n_cols,figsize=(n_cols*3.3,n_rows*3.3),sharey=True)\n",
    "#     #fig.suptitle('Model: '+name)\n",
    "#     plt.rcParams.update({'font.size': 6,'axes.labelsize':8,'xtick.labelsize':8,'ytick.labelsize':8})\n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     for k1,v1 in best_params1.items():\n",
    "#         k_dict = {}\n",
    "#         for k,v in best_params1.items():\n",
    "#             if (k==k1):\n",
    "#                 k_dict[k]=param_grid_big[k]\n",
    "#                # k_dict[k]=param_grid[k]\n",
    "#             else:\n",
    "#                 k_dict[k] = [best_params1[k]]\n",
    "#         rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "#         optimizer = model_selection.GridSearchCV(rf, k_dict, cv=5, n_jobs=4,return_train_score=True)\n",
    "#         optimizer.fit(X, y)\n",
    "#         best_params2[k1] = optimizer.best_params_[k1]\n",
    "#         if (k1 in ['n_estimators','min_impurity_decrease']):\n",
    "#             plot_scores(optimizer,k1,axs[i][j],log=True)\n",
    "#             axs[i][j].title.set_text(k1)\n",
    "#         else:\n",
    "#             plot_scores(optimizer,k1,axs[i][j],log=False)\n",
    "#             axs[i][j].title.set_text(k1)\n",
    "#         j = j + 1\n",
    "#         if (j==axs.shape[1]):\n",
    "#             i = i + 1\n",
    "#             j = 0\n",
    "#     #plt.show()\n",
    "#     fig.add_subplot(111, frameon=False)\n",
    "#     plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "#     plt.grid(b=None)\n",
    "#     #plt.xlabel(\"common X\")\n",
    "#     plt.ylabel(\"Accuracy\",labelpad=10)\n",
    "#     plt.savefig('pics/test_params_'+name+'.png', dpi = 300, bbox_inches='tight')\n",
    "#     print('Best parameters 2',best_params2)\n",
    "#     return(best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameters(X,y,name,param_pic_name,pdf):\n",
    "    features_sqrt = int(np.sqrt(X.shape[1]))+1\n",
    "    param_grid = {'n_estimators': [1,5,10,50,100,500],\n",
    "                      'max_depth': np.arange(2,20,1),\n",
    "                      'min_samples_split': np.arange(2,20,1),\n",
    "                      'max_features': np.arange(1,features_sqrt+1)\n",
    "                     }\n",
    "#     param_grid = {'n_estimators': [1,5],\n",
    "#                       'max_depth': np.arange(2,3,1),\n",
    "#                       'min_samples_split': np.arange(2,3,1),\n",
    "#                       'max_features': np.arange(1,X.shape[1]+1)\n",
    "#                  }\n",
    "    rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "    optimizer = model_selection.GridSearchCV(rf, param_grid, cv=5, n_jobs=4,return_train_score=True)\n",
    "    optimizer.fit(X, y)\n",
    "    best_score1 = optimizer.best_score_\n",
    "    best_params1 = optimizer.best_params_\n",
    "    print('Best score 1:',best_score1)\n",
    "    print('Best parameters 1:',best_params1)\n",
    "    n_cols = 2\n",
    "    n_rows = int(np.ceil((len(best_params1))/n_cols))\n",
    "    fig, axs = plt.subplots(n_rows,n_cols,figsize=(n_cols*3.3,n_rows*3.3),sharey=True)\n",
    "    plt.rcParams.update({'font.size': 6,'axes.labelsize':8,'xtick.labelsize':8,'ytick.labelsize':8})\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for k1,v1 in best_params1.items():\n",
    "        if (k1 in ['n_estimators','min_impurity_decrease']):\n",
    "            plot_scores(optimizer,k1,axs[i][j],log=True)\n",
    "            axs[i][j].title.set_text(k1)\n",
    "        else:\n",
    "            plot_scores(optimizer,k1,axs[i][j],log=False)\n",
    "            axs[i][j].title.set_text(k1)\n",
    "        j = j + 1\n",
    "        if (j==axs.shape[1]):\n",
    "            i = i + 1\n",
    "            j = 0\n",
    "    #plt.show()\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    plt.grid(b=None)\n",
    "    #plt.xlabel(\"common X\")\n",
    "    plt.ylabel(\"Accuracy\",labelpad=10)\n",
    "    plt.savefig(param_pic_name, dpi = 300, bbox_inches='tight')\n",
    "    return(best_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(optimizer,p_name,ax,log=False):\n",
    "    scores=[]\n",
    "    for i in range(len(optimizer.cv_results_['params'])):\n",
    "        scores.append([optimizer.cv_results_['params'][i][p_name], \n",
    "                optimizer.cv_results_['mean_test_score'][i],\n",
    "                optimizer.cv_results_['std_test_score'][i]])\n",
    "    scores = np.array(scores)\n",
    "    if (log):\n",
    "        ax.semilogx(scores[:,0], scores[:,1],label='test')\n",
    "    else:\n",
    "        ax.plot(scores[:,0], scores[:,1],label='test')\n",
    "    ax.fill_between(scores[:,0], scores[:,1]-scores[:,2], \n",
    "                                  scores[:,1]+scores[:,2], alpha=0.3)\n",
    "    \n",
    "    scores2=[]\n",
    "    for i in range(len(optimizer.cv_results_['params'])):\n",
    "        scores2.append([optimizer.cv_results_['params'][i][p_name], \n",
    "                optimizer.cv_results_['mean_train_score'][i],\n",
    "                optimizer.cv_results_['std_train_score'][i]])\n",
    "    scores2 = np.array(scores2)\n",
    "    if (log):\n",
    "        ax.semilogx(scores2[:,0], scores2[:,1],label='train')\n",
    "    else:\n",
    "        ax.plot(scores2[:,0], scores2[:,1],label='train')\n",
    "    ax.fill_between(scores2[:,0], scores2[:,1]-scores2[:,2], \n",
    "                                  scores2[:,1]+scores2[:,2], alpha=0.3)\n",
    "    ax.legend(loc = 'lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cv(rf,X,y,name):\n",
    "    sss = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=0)\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    p_names = ['accuracy','precision','recall','f1','auc','tpr']\n",
    "    tt = ['train','test']\n",
    "    cv_results = {}\n",
    "    for p,t in itertools.product(p_names, tt):\n",
    "        cv_results[(p,t)]=[]\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rf.fit(X_train,y_train)\n",
    "        y_predicted = rf.predict(X_test)\n",
    "        y_predicted_train = rf.predict(X_train)\n",
    "        y_proba = rf.predict_proba(X_test).transpose()[1]\n",
    "        y_proba_train = rf.predict_proba(X_train).transpose()[1]\n",
    "        \n",
    "        cv_results[('accuracy','test')].append(metrics.accuracy_score(y_test, y_predicted))\n",
    "        cv_results[('accuracy','train')].append(metrics.accuracy_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('f1','test')].append(metrics.f1_score(y_test, y_predicted))\n",
    "        cv_results[('f1','train')].append(metrics.f1_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('precision','test')].append(metrics.precision_score(y_test, y_predicted))\n",
    "        cv_results[('precision','train')].append(metrics.precision_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('recall','test')].append(metrics.recall_score(y_test, y_predicted))\n",
    "        cv_results[('recall','train')].append(metrics.recall_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('auc','test')].append(metrics.roc_auc_score(y_test, y_proba))\n",
    "        cv_results[('auc','train')].append(metrics.roc_auc_score(y_train, y_proba_train))\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_proba)\n",
    "        fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_proba_train)\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr_train = np.interp(base_fpr, fpr_train, tpr_train)\n",
    "        tpr[0] = 0.0\n",
    "        tpr_train[0] = 0.0\n",
    "        cv_results[('tpr','test')].append(tpr)\n",
    "        cv_results[('tpr','train')].append(tpr_train)\n",
    "    col = {'train':'c','test':'m'}\n",
    "    for t in tt:\n",
    "        t_tprs = np.array(cv_results[('tpr',t)])\n",
    "        mean = t_tprs.mean(axis=0)\n",
    "        std = t_tprs.std(axis=0)\n",
    "        \n",
    "        mean_auc = np.mean(cv_results[('auc',t)])\n",
    "        std_auc = np.std(cv_results[('auc',t)])\n",
    "        \n",
    "        tprs_upper = np.minimum(mean + std, 1)\n",
    "        tprs_lower = mean - std\n",
    "        \n",
    "        plt.plot(base_fpr, mean, color = col[t], alpha = 0.8, label=t+r' (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc))\n",
    "        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = col[t], alpha = 0.2)\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    p_names = ['accuracy','precision','recall','f1','auc']\n",
    "    res = {}\n",
    "    for p in p_names:\n",
    "        mean = np.mean(cv_results[(p,t)])\n",
    "        std = np.std(cv_results[(p,t)])\n",
    "        mean_std = '%.3f ± %.3f' % (mean, std)\n",
    "        print(p,mean_std)\n",
    "        res[(p+'_cv')] = mean_std\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cv2(rf,X,y,name,pic_name,ax,feature,clr_b,pdf):\n",
    "    sss = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=0)\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    p_names = ['accuracy','precision','recall','f1','auc','tpr']\n",
    "    tt = ['train','test']\n",
    "    cv_results = {}\n",
    "    for p,t in itertools.product(p_names, tt):\n",
    "        cv_results[(p,t)]=[]\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rf.fit(X_train,y_train)\n",
    "        y_predicted = rf.predict(X_test)\n",
    "        y_predicted_train = rf.predict(X_train)\n",
    "        y_proba = rf.predict_proba(X_test).transpose()[1]\n",
    "        y_proba_train = rf.predict_proba(X_train).transpose()[1]\n",
    "        \n",
    "        cv_results[('accuracy','test')].append(metrics.accuracy_score(y_test, y_predicted))\n",
    "        cv_results[('accuracy','train')].append(metrics.accuracy_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('f1','test')].append(metrics.f1_score(y_test, y_predicted))\n",
    "        cv_results[('f1','train')].append(metrics.f1_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('precision','test')].append(metrics.precision_score(y_test, y_predicted))\n",
    "        cv_results[('precision','train')].append(metrics.precision_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('recall','test')].append(metrics.recall_score(y_test, y_predicted))\n",
    "        cv_results[('recall','train')].append(metrics.recall_score(y_train, y_predicted_train))\n",
    "        \n",
    "        cv_results[('auc','test')].append(metrics.roc_auc_score(y_test, y_proba))\n",
    "        cv_results[('auc','train')].append(metrics.roc_auc_score(y_train, y_proba_train))\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_proba)\n",
    "        fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_proba_train)\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr_train = np.interp(base_fpr, fpr_train, tpr_train)\n",
    "        tpr[0] = 0.0\n",
    "        tpr_train[0] = 0.0\n",
    "        cv_results[('tpr','test')].append(tpr)\n",
    "        cv_results[('tpr','train')].append(tpr_train)\n",
    "    col = {'all':'orange','best_fdr':'c','best_holm':'m'}\n",
    "    lstyle = {True:'dashed',False:'solid'}\n",
    "    col_fig1 = {'train':'c','test':'m'}\n",
    "    plt.figure(figsize=(3.3,3.3))\n",
    "    for t in tt:\n",
    "        t_tprs = np.array(cv_results[('tpr',t)])\n",
    "        mean = t_tprs.mean(axis=0)\n",
    "        std = t_tprs.std(axis=0)\n",
    "        \n",
    "        mean_auc = np.mean(cv_results[('auc',t)])\n",
    "        std_auc = np.std(cv_results[('auc',t)])\n",
    "        \n",
    "        tprs_upper = np.minimum(mean + std, 1)\n",
    "        tprs_lower = mean - std\n",
    "        \n",
    "        plt.plot(base_fpr, mean, color = col_fig1[t], alpha = 0.8, label=t+r' (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc))\n",
    "        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = col_fig1[t], alpha = 0.2)\n",
    "        \n",
    "        if ((t == 'test')):\n",
    "            name1 = '_'.join(name.split('_')[1:])\n",
    "            ax.plot(base_fpr, mean, color = col[feature], linestyle=lstyle[clr_b],alpha = 0.8, label=name1+r' (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc))\n",
    "        #plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = col[t], alpha = 0.2)\n",
    "    ax.legend(frameon=False)\n",
    "    plt.legend()\n",
    "    plt.savefig(pic_name, dpi = 300, bbox_inches='tight')\n",
    "    p_names = ['accuracy','precision','recall','f1','auc']\n",
    "    res = {}\n",
    "    for p in p_names:\n",
    "        mean = np.mean(cv_results[(p,t)])\n",
    "        std = np.std(cv_results[(p,t)])\n",
    "        mean_std = '%.3f ± %.3f' % (mean, std)\n",
    "        print(p,mean_std)\n",
    "        res[(p+'_cv')] = mean_std\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oob_estimate(X,y,bp):\n",
    "    oob_df_list = []\n",
    "    for i in range(100):\n",
    "        rf = ensemble.RandomForestClassifier(random_state=i,oob_score=True)\n",
    "        rf.set_params(**bp)\n",
    "        rf.fit(X,y)\n",
    "        prob_0=rf.oob_decision_function_.transpose()[0]\n",
    "        y_predicted = [0 if x>0.5 else 1 for x in prob_0]\n",
    "        oob_df_list.append({'oob_accuracy':metrics.accuracy_score(y,y_predicted),\n",
    "                            'oob_f1_score':metrics.f1_score(y,y_predicted),\n",
    "                            'oob_precision':metrics.precision_score(y,y_predicted),\n",
    "                            'oob_recall':metrics.recall_score(y,y_predicted)})\n",
    "    oob_df = pd.DataFrame(oob_df_list)\n",
    "    means = oob_df.mean(axis=0)\n",
    "    stds = oob_df.std(axis=0)\n",
    "    oob_df_avg = pd.DataFrame.from_dict({'means':means,'stds':stds})\n",
    "    oob_df_avg['mean_std'] = oob_df_avg.apply(lambda x: ('%.3f ± %.3f' % (x.means, x.stds)),axis=1)\n",
    "    return(oob_df_avg['mean_std'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters search, best parametrs model CV on train, and evaluation on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Family_ALL (tax level: Family ; features: all ; CLR: False )\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = pd.DataFrame()\n",
    "levels = [4]\n",
    "features = ['all']\n",
    "clr_b = [False]\n",
    "#levels = [4,5]\n",
    "#features = ['all','best_fdr','best_holm']\n",
    "#clr_b = [True,False]\n",
    "fig,axes = plt.subplots(1,2,figsize=(6.6,3.3),sharex=True,sharey=True)\n",
    "axes[0].set_title('Family models')\n",
    "axes[1].set_title('Genus models')\n",
    "ax_dict = {4:axes[0],5:axes[1]}\n",
    "pdf = fpdf.FPDF()\n",
    "pdf.set_font(\"Arial\", size=20)\n",
    "pdf.add_page()\n",
    "pdf.cell(200, 10, txt=\"Supplementary Text 1\", align=\"C\")\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "pdf.ln(20)\n",
    "pdf.cell(200, 20, txt='text',\n",
    "         align=\"L\")\n",
    "for level,feature,clr_b in itertools.product(levels, features, clr_b):\n",
    "    view_name = shared_functions.view_name(level,feature,clr_b)\n",
    "    print('Model:',view_name,'(tax level:',shared_functions.get_tax_name_by_level(level),\n",
    "          '; features:',feature,'; CLR:',clr_b,')')\n",
    "    f,chao = bf[(level,feature)]\n",
    "    tf = shared_functions.transformer(bf=f,chao=chao,level=level,clr_b=clr_b)\n",
    "    taxa_df,chao_df,y = data.get_data_from_ind(dataset_info.index,level,False)\n",
    "    X = tf.transform_df(taxa_df,chao_df)\n",
    "\n",
    "    pdf.add_page()\n",
    "    pdf.cell(200, 10, txt='Model: '+view_name, align=\"C\")\n",
    "    pdf.ln(20)\n",
    "    param_pic_name = os.path.join(pic_dir,'test_params_'+view_name+'.png')\n",
    "    bp = test_parameters(X,y,view_name,param_pic_name,pdf)\n",
    "    pdf.cell(200, 10, txt=\"Parameters selection:\", align=\"L\")\n",
    "    pdf.image(param_pic_name, x=20, y=45, w=140)\n",
    "    os.remove(param_pic_name)\n",
    "    #bp = {'max_depth': 11, 'max_features': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "    rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "    rf.set_params(**bp)\n",
    "    name = shared_functions.make_name(level,feature,clr_b,False)\n",
    "    joblib.dump(rf, os.path.join(joblib_dir,name))\n",
    "    pdf.ln(155)\n",
    "    pdf.cell(200, 10, txt=\"ROC curve:\", align=\"L\")\n",
    "    roc_pic_name = os.path.join(pic_dir,'test_roc_'+view_name+'.png')\n",
    "    res_dict = test_cv2(rf,X,y,view_name,roc_pic_name,ax_dict[level],feature,clr_b,pdf)\n",
    "    pdf.image(roc_pic_name, x=20, y=205, w=85)\n",
    "    os.remove(roc_pic_name)\n",
    "    rf.fit(X,y)\n",
    "    name = shared_functions.make_name(level,feature,clr_b,True)\n",
    "    joblib.dump(rf, os.path.join(joblib_dir,name))\n",
    "    oob_res_dict = get_oob_estimate(X,y,bp)\n",
    "    print(oob_res_dict)\n",
    "    p = {'name':view_name,'level':shared_functions.get_tax_name_by_level(level),'features':feature,'features_n':X.shape[1],'CLR':clr_b}\n",
    "    z = {**p, **bp, **res_dict,**oob_res_dict}\n",
    "    res_i = pd.DataFrame(z,index=[0])\n",
    "    res = pd.concat([res,res_i])\n",
    "    print('*****************')\n",
    "#plt.show()\n",
    "#pdf.output(\"pics/SupplementaryText1.pdf\")\n",
    "#fig.savefig('pics/SupplementaryFigure3.pdf', dpi = 300, bbox_inches='tight')\n",
    "#res.to_csv('results/catsNdogs_SupplementaryTable4.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
